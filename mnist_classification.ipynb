{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MNIST Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üéØ <b><u>Exercise objectives</u></b>\n",
    "- Understand the *MNIST* dataset \n",
    "- Design your first **Convolutional Neural Network** (*CNN*) and answer questions such as:\n",
    "    - what are *Convolutional Layers*? \n",
    "    - how many *parameters* are involved in such a layer?\n",
    "- Train this CNN on images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üöÄ <b><u>Let's get started!</u></b>\n",
    "\n",
    "Imagine that we are  back in time into the 90's.\n",
    "You work at a *Post Office* and you have to deal with an enormous amount of letters on a daily basis. How could you automate the process of reading the ZIP Codes, which are a combination of 5 handwritten digits? \n",
    "\n",
    "This task, called the **Handwriting Recognition**, used to be a very complex problem back in those days. It was solved by *Bell Labs* (among others) where one of the Deep Learning gurus, [*Yann Le Cun*](https://en.wikipedia.org/wiki/Yann_LeCun), used to work.\n",
    "\n",
    "From [Wikipedia](https://en.wikipedia.org/wiki/Handwriting_recognition):\n",
    "\n",
    "> Handwriting recognition (HWR), also known as Handwritten Text Recognition (HTR), is the ability of a computer to receive and interpret intelligible handwritten input from sources such as paper documents, photographs, touch-screens and other devices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Number recognition](recognition.gif)\n",
    "\n",
    "*Note: The animation above is just here to help you visualize what happens with the different images: <br/> $\\rightarrow$ For each image, once the CNN is trained, it will predict what digit is written. The inputs are the different digits and not one animation/video!*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ü§î <b><u>How does this CNN work ?</u></b>\n",
    "\n",
    "- *Inputs*: Images (_each image shows a handwritten digit_)\n",
    "- *Target*: For each image, you want your CNN model to predict the correct digit (between 0 and 9)\n",
    "    - It is a **multi-class classification** task (more precisely a 10-class classification task since there are 10 different digits).\n",
    "\n",
    "üî¢ To improve the capacity of the Convolutional Neural Network to read these numbers, we need to feed it with many images representing handwritten digits. This is why the üìö [**MNIST dataset**](http://yann.lecun.com/exdb/mnist/) *(Mixed National Institute of Standards and Technology)* was created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (1) The `MNIST` Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üìö Tensorflow/Keras offers multiple [**datasets**](https://www.tensorflow.org/api_docs/python/tf/keras/datasets) to play with:\n",
    "- *Vectors*: `boston_housing` (regression)\n",
    "- *Images* : `mnist`, `fashion_mnist`, `cifar10`, `cifar100` (classification)\n",
    "- *Texts*: `imbd`, `reuters` (classification/sentiment analysis)\n",
    "\n",
    "\n",
    "üíæ You can **load the MNIST dataset** with the following commands:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-21 11:36:19.569735: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(((60000, 28, 28), (60000,)), ((10000, 28, 28), (10000,)))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.keras import datasets\n",
    "\n",
    "\n",
    "# Loading the MNIST Dataset...\n",
    "(X_train, y_train), (X_test, y_test) = datasets.mnist.load_data(path=\"mnist.npz\")\n",
    "\n",
    "# The train set contains 60 000 images, each of them of size 28x28\n",
    "# The test set contains 10 000 images, each of them of size 28x28\n",
    "(X_train.shape, y_train.shape), (X_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (1.1) Exploring the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ùì **Question: Let's have look at some handwritten digits of this MNIST dataset.** ‚ùì\n",
    "\n",
    "üñ® Print some images from the *train set*.\n",
    "\n",
    "<details>\n",
    "    <summary><i>Hints</i></summary>\n",
    "\n",
    "üí°*Hint*: use the `imshow` function from `matplotlib` with `cmap = \"gray\"`\n",
    "\n",
    "ü§® Note: if you don't specify this *cmap* argument, the weirdly displayed colors are just Matplotlib defaults...\n",
    "    \n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": [
     "challengify"
    ]
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1000x500 with 5 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.datasets import mnist\n",
    "\n",
    "# Load the MNIST dataset\n",
    "(train_images, _), (_, _) = mnist.load_data()\n",
    "\n",
    "# Plot the first 5 images\n",
    "plt.figure(figsize=(10, 5))\n",
    "for i in range(5):\n",
    "    plt.subplot(1, 5, i + 1)\n",
    "    plt.imshow(train_images[i], cmap='gray')\n",
    "    plt.axis('off')  # Turn off axis\n",
    "    plt.title(f'Digit: {_[i]}')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (1.2) Image Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ùóÔ∏è **Neural Networks converge faster when the input data is somehow normalized** ‚ùóÔ∏è\n",
    "\n",
    "üë©üèª‚Äçüè´ How do we proceed for Convolutional Neural Networks ?\n",
    "* The `RBG` intensities are coded between 0 and 255. \n",
    "* We can simply divide the input data by the maximal value 255 to have all the pixels' intensities between 0 and 1 üòâ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ùì **Question ‚ùì As a first preprocessing step, please normalize your data.** \n",
    "\n",
    "Don't forget to do it both on your train data and your test data.\n",
    "\n",
    "(*Note: you can also center your data, by subtracting 0.5 from all the values, but it is not mandatory*)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": [
     "challengify"
    ]
   },
   "outputs": [],
   "source": [
    "X_train_normalized = X_train / 255.0\n",
    "\n",
    "X_test_normalized = X_test / 255.0\n",
    "\n",
    "X_train_normalized = X_train / 255.0\n",
    "\n",
    "X_test_normalized = X_test / 255.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (1.3) Inputs' dimensionality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 28, 28)\n",
      "(10000, 28, 28)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape)\n",
    "print(X_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üëÜ Remember that you have 60,000 training images and 10,000 test images, each of size $(28, 28)$. However...\n",
    "\n",
    "> ‚ùóÔ∏è  **`Convolutional Neural Network models need to be fed with images whose last dimension is the number of channels`.**  \n",
    "\n",
    "> üßëüèª‚Äçüè´ The shape of tensors fed into ***ConvNets*** is the following: `(NUMBER_OF_IMAGES, HEIGHT, WIDTH, CHANNELS)`\n",
    "\n",
    "üïµüèªThis last dimension is clearly missing here. Can you guess the reason why?\n",
    "<br>\n",
    "<details>\n",
    "    <summary><i>Answer<i></summary>\n",
    "        \n",
    "* All these $60000$ $ (28 \\times 28) $ pictures are black-and-white $ \\implies $ Each pixel lives on a spectrum from full black (0) to full white (1).\n",
    "        \n",
    "    * Theoretically, you don't need to know the number of channels for a black-and-white picture since there is only 1 channel (the \"whiteness\" of \"blackness\" of a pixel). However, it is still mandatory for the model to have this number of channels explicitly stated.\n",
    "        \n",
    "    * In comparison, colored pictures need multiple channels:\n",
    "        - the RGB system with 3 channels (<b><span style=\"color:red\">Red</span> <span style=\"color:green\">Green</span> <span style=\"color:blue\">Blue</span></b>)\n",
    "        - the CYMK system  with 4 channels (<b><span style=\"color:cyan\">Cyan</span> <span style=\"color:magenta\">Magenta</span> <span style=\"color:yellow\">Yellow</span> <span style=\"color:black\">Black</span></b>)\n",
    "        \n",
    "        \n",
    "</details>        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ùì **Question: expanding dimensions** ‚ùì\n",
    "\n",
    "* Use the **`expand_dims`** to add one dimension at the end of the training data and test data.\n",
    "\n",
    "* Then, print the shapes of `X_train` and `X_test`. They should respectively be equal to $(60000, 28, 28, 1)$ and $(10000, 28, 28, 1)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.backend import expand_dims"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X_train: (60000, 28, 28, 1)\n",
      "Shape of X_test: (10000, 28, 28, 1)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Assuming you have your normalized data stored in X_train_normalized and X_test_normalized\n",
    "# X_train_normalized and X_test_normalized are your input images after normalization\n",
    "\n",
    "# Add a channel dimension at the end of the training data\n",
    "X_train_expanded = np.expand_dims(X_train_normalized, axis=-1)\n",
    "\n",
    "# Add a channel dimension at the end of the test data\n",
    "X_test_expanded = np.expand_dims(X_test_normalized, axis=-1)\n",
    "\n",
    "# Print the shapes of X_train and X_test\n",
    "print(\"Shape of X_train:\", X_train_expanded.shape)  # (60000, 28, 28, 1)\n",
    "print(\"Shape of X_test:\", X_test_expanded.shape)    # (10000, 28, 28, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (1.4) Target encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One more thing to do for a multiclass classification task in Deep Leaning:\n",
    "\n",
    "üëâ _\"one-hot-encode\" the categories*_\n",
    "\n",
    "‚ùì **Question: encoding the labels** ‚ùì \n",
    "\n",
    "* Use **`to_categorical`** to transform your labels. \n",
    "* Store the results into two variables that you can call **`y_train_cat`** and **`y_test_cat`**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "\n",
    "y_train_cat = to_categorical(y_train)\n",
    "\n",
    "# Perform one-hot encoding for the test labels\n",
    "y_test_cat = to_categorical(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick check that you correctly used to_categorical\n",
    "assert(y_train_cat.shape == (60000,10))\n",
    "assert(y_test_cat.shape == (10000,10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data is now ready to be used. ‚úÖ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (2) The Convolutional Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (2.1) Architecture and compilation of a CNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "‚ùì **Question: CNN Architecture and compilation** ‚ùì\n",
    "\n",
    "Now, let's build a <u>Convolutional Neural Network</u> that has: \n",
    "\n",
    "\n",
    "- a `Conv2D` layer with 8 filters, each of size $(4, 4)$, an input shape suitable for your task, the `relu` activation function, and `padding='same'`\n",
    "- a `MaxPool2D` layer with a `pool_size` equal to $(2, 2)$\n",
    "- a second `Conv2D` layer with 16 filters, each of size $(3, 3)$, and the `relu` activation function\n",
    "- a second `MaxPool2D` layer with a `pool_size` equal to $(2, 2)$\n",
    "\n",
    "\n",
    "- a `Flatten` layer\n",
    "- a first `Dense` layer with 10 neurons and the `relu` activation function\n",
    "- a last (predictive) layer that is suited for your task\n",
    "\n",
    "In the function that initializes this model, do not forget to include the <u>compilation of the model</u>, which:\n",
    "* optimizes the `categorical_crossentropy` loss function,\n",
    "* with the `adam` optimizer, \n",
    "* and the `accuracy` as the metrics\n",
    "\n",
    "(*Note: you could add more classification metrics if you want but the dataset is well balanced!*)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d_6 (Conv2D)           (None, 28, 28, 8)         136       \n",
      "                                                                 \n",
      " conv2d_7 (Conv2D)           (None, 26, 26, 16)        1168      \n",
      "                                                                 \n",
      " max_pooling2d_3 (MaxPooling  (None, 13, 13, 16)       0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " flatten_3 (Flatten)         (None, 2704)              0         \n",
      "                                                                 \n",
      " dense_6 (Dense)             (None, 10)                27050     \n",
      "                                                                 \n",
      " dense_7 (Dense)             (None, 10)                110       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 28,464\n",
      "Trainable params: 28,464\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import models\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "\n",
    "input_shape = (28, 28, 1)  # Example input shape for MNIST dataset\n",
    "num_classes = 10  # Example number of classes for MNIST dataset\n",
    "\n",
    "model = models.Sequential()\n",
    "\n",
    "    ### First Convolution & MaxPooling\n",
    "model.add(Conv2D(8,(4,4), activation='relu', padding='same', input_shape=input_shape))\n",
    "    \n",
    "    ### Second Convolution & MaxPooling\n",
    "model.add(Conv2D(16,(3,3), activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "    \n",
    "    \n",
    "    ### Flattening\n",
    "model.add(Flatten())\n",
    "    \n",
    "    ### One Fully Connected layer - \"Fully Connected\" is equivalent to saying \"Dense\"\n",
    "model.add(Dense(10, activation='relu'))\n",
    "    \n",
    "    ### Last layer - Classification Layer with 10 outputs corresponding to 10 digits\n",
    "model.add(Dense(num_classes, activation='softmax'))\n",
    "    \n",
    "    ### Model compilation\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "                  optimizer='adam',\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "    \n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ùì **Question: number of trainable parameters in a convolutional layer** ‚ùì \n",
    "\n",
    "How many trainable parameters are there in your model?\n",
    "1. Compute them with ***model.summary( )*** first\n",
    "2. Recompute them manually to make sure you properly understood ***what influences the number of weights in a CNN***."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "tags": [
     "challengify"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<keras.engine.sequential.Sequential at 0x7f560699e620>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "initialize_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (2.2) Training a CNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ùì **Question: training a CNN** ‚ùì \n",
    "\n",
    "Initialize your model and fit it on the train data. \n",
    "- Do not forget to use a **Validation Set/Split** and an **Early Stopping criterion**. \n",
    "- Limit yourself to 5 epochs max in this challenge, just to save some precious time for the more advanced challenges!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "1500/1500 [==============================] - 7s 5ms/step - loss: 0.3071 - accuracy: 0.9078 - val_loss: 0.1325 - val_accuracy: 0.9634\n",
      "Epoch 2/5\n",
      "1500/1500 [==============================] - 6s 4ms/step - loss: 0.1034 - accuracy: 0.9683 - val_loss: 0.0790 - val_accuracy: 0.9770\n",
      "Epoch 3/5\n",
      "1500/1500 [==============================] - 7s 4ms/step - loss: 0.0708 - accuracy: 0.9786 - val_loss: 0.0724 - val_accuracy: 0.9783\n",
      "Epoch 4/5\n",
      "1500/1500 [==============================] - 6s 4ms/step - loss: 0.0559 - accuracy: 0.9829 - val_loss: 0.0621 - val_accuracy: 0.9825\n",
      "Epoch 5/5\n",
      "1500/1500 [==============================] - 6s 4ms/step - loss: 0.0463 - accuracy: 0.9864 - val_loss: 0.0573 - val_accuracy: 0.9833\n",
      "313/313 [==============================] - 1s 2ms/step - loss: 0.0488 - accuracy: 0.9840\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "\n",
    "# Define early stopping criteria\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
    "\n",
    "# Fit the model on the train data with validation split\n",
    "history = model.fit(X_train_normalized, y_train_cat, \n",
    "                    batch_size=32, \n",
    "                    epochs=5, \n",
    "                    validation_split=0.2, \n",
    "                    callbacks=[early_stopping])\n",
    "\n",
    "# Evaluate the model on the test data\n",
    "test_loss, test_accuracy = model.evaluate(X_test_normalized, y_test_cat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ùì **Question: How many iterations does the CNN perform per epoch** ‚ùì\n",
    "\n",
    "_Note: it has nothing to do with the fact that this is a CNN. This is related to the concept of forward/backward propagation already covered during the previous lecture on optimizers, fitting, and losses üòâ_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "challengify"
    ]
   },
   "source": [
    "> YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "    <summary><i>Answer</i></summary>\n",
    "\n",
    "With `verbose = 1` when fitting your model, you have access to crucial information about your training procedure.\n",
    "    \n",
    "Remember that we've just trained our CNN model on $60000$ training images\n",
    "\n",
    "If the chosen batch size is 32: \n",
    "\n",
    "* For each epoch, we have $ \\large \\lceil \\frac{60000}{32} \\rceil = 1875$ minibatches <br/>\n",
    "* The _validation_split_ is equal to $0.3$ - which means that within one single epoch, there are:\n",
    "    * $ \\lceil 1875 \\times (1 - 0.3) \\rceil = \\lceil 1312.5 \\rceil = 1313$ batches are used to compute the `train_loss` \n",
    "    * $ 1875 - 1312 = 562 $ batches are used to compute the `val_loss`\n",
    "    * **The parameters are updated 1313 times per epoch** as there are 1313 forward/backward propagations per epoch !!!\n",
    "\n",
    "\n",
    "üëâ With so many updates of the weights within one epoch, you can understand why this CNN model converges even with a limited number of epochs.\n",
    "\n",
    "</details>    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (2.3) Evaluating its performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ùì **Question: Evaluating your CNN** ‚ùì \n",
    "\n",
    "What is your **`accuracy on the test set?`**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "challengify"
    ]
   },
   "outputs": [],
   "source": [
    "0.9840"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üéâ You should already be impressed by your CNN skills! Reaching over 95% accuracy!\n",
    "\n",
    "üî• You solved what was a very hard problem 30 years ago with your own CNN."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üèÅ **Congratulations!**\n",
    "\n",
    "üíæ Don't forget to `git add/commit/push` your notebook...\n",
    "\n",
    "üöÄ ... and move on to the next challenge!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
